{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downstream adaption with MiniMol\n",
    "\n",
    "This example shows how MiniMol can featurise the molecules that will then serve as an input to another model trained on a small downstream dataset from TDC ADMET. This allows to transfer the knowledge from the pre-trained MiniMol to another task. \n",
    "\n",
    "Before we start, let's make sure that the TDC package is installed in the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install PyTDC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Getting the data\n",
    "Next, we will build a predictor for the `HIA Hou` dataset, one of the binary classification benchmarks corresponding to `absorption`-type of problems from TDC ADMET group. We then split the data into training, validation and test set based on molecular scaffolds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "generating training, validation splits...\n",
      "generating training, validation splits...\n",
      "100%|██████████| 461/461 [00:00<00:00, 3650.21it/s]\n"
     ]
    }
   ],
   "source": [
    "from tdc.benchmark_group import admet_group\n",
    "\n",
    "DATASET_NAME = 'hia_hou'\n",
    "\n",
    "admet = admet_group(path=\"admet-data/\")\n",
    "\n",
    "mols_test = admet.get(DATASET_NAME)['test']\n",
    "mols_train, mols_val = admet.get_train_valid_split(benchmark=DATASET_NAME, split_type='scaffold', seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset - hia_hou\n",
      "\n",
      "Val split (58 mols): \n",
      "                 Drug_ID                                               Drug  Y\n",
      "0         Atracurium.mol  COc1ccc(C[C@H]2c3cc(OC)c(OC)cc3CC[N@@+]2(C)CCC...  0\n",
      "1  Succinylsulfathiazole          O=C(O)CCC(=O)Nc1ccc(S(=O)(=O)Nc2nccs2)cc1  0\n",
      "2            Ticarcillin  CC1(C)S[C@H]2[C@@H](NC(=O)[C@@H](C(=O)O)c3ccsc...  0\n",
      "3          Raffinose.mol  OC[C@@H]1O[C@@H](OC[C@@H]2O[C@@H](O[C@]3(CO)O[...  0\n",
      "4          Triamcinolone  C[C@@]12C=CC(=O)C=C1CC[C@@H]1[C@H]3C[C@@H](O)[...  1\n",
      "\n",
      "Test split (117 mols): \n",
      "                Drug_ID                                               Drug  Y\n",
      "0         Trazodone.mol         O=c1n(CCCN2CCN(c3cccc(Cl)c3)CC2)nc2ccccn12  1\n",
      "1          Lisuride.mol  CCN(CC)C(=O)N[C@H]1C=C2c3cccc4[nH]cc(c34)C[C@@...  1\n",
      "2  Methylergonovine.mol  CC[C@H](CO)NC(=O)[C@H]1C=C2c3cccc4[nH]cc(c34)C...  1\n",
      "3      Methysergide.mol  CC[C@H](CO)NC(=O)[C@H]1C=C2c3cccc4c3c(cn4C)C[C...  1\n",
      "4       Moclobemide.mol                       O=C(NCCN1CCOCC1)c1ccc(Cl)cc1  1\n",
      "\n",
      "Train split (403 mols): \n",
      "           Drug_ID                                               Drug  Y\n",
      "0        Guanadrel                      N=C(N)NC[C@@H]1COC2(CCCCC2)O1  1\n",
      "1      Cefmetazole  CO[C@@]1(NC(=O)CSCC#N)C(=O)N2C(C(=O)O)=C(CSc3n...  0\n",
      "2   Zonisamide.mol                           NS(=O)(=O)Cc1noc2ccccc12  1\n",
      "3   Furosemide.mol            NS(=O)(=O)c1cc(Cl)cc(NCc2ccco2)c1C(=O)O  1\n",
      "4  Telmisartan.mol  CCCc1nc2c(n1Cc1ccc(-c3ccccc3C(=O)O)cc1)=C[C@H]...  1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset - {DATASET_NAME}\\n\")\n",
    "print(f\"Val split ({len(mols_val)} mols): \\n{mols_val.head()}\\n\")\n",
    "print(f\"Test split ({len(mols_test)} mols): \\n{mols_test.head()}\\n\")\n",
    "print(f\"Train split ({len(mols_train)} mols): \\n{mols_train.head()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generating molecular fingerprints\n",
    "After spltting the dataset into training, validation and test sets, we will use MiniMol to embed all molecules. The embedding will be added as an extra column in the dataframe returned by TDC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minimol import Minimol\n",
    "\n",
    "model = Minimol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5d0efb3f0dc45aeb3522d3ab10f92ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "featurizing_smiles, batch=1:   0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting to FP32: 100%|██████████| 58/58 [00:00<00:00, 16144.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.06it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e182ff345e4a05b467f5cbe2edec38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "featurizing_smiles, batch=3:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting to FP32: 100%|██████████| 117/117 [00:00<00:00, 17139.94it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  6.25it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1178af214fd844ce9d2182486d88e4d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "featurizing_smiles, batch=13:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting to FP32: 100%|██████████| 403/403 [00:00<00:00, 12412.10it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00,  7.30it/s]\n"
     ]
    }
   ],
   "source": [
    "mols_val['Embedding'] = model(mols_val['Drug'])\n",
    "mols_test['Embedding'] = model(mols_test['Drug'])\n",
    "mols_train['Embedding'] = model(mols_train['Drug'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is small, so it took us 7.3 seconds to generate the embeddings for almost 600 molecules. Here is a preview after a new column has been added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Drug_ID                                               Drug  Y  \\\n",
      "0        Guanadrel                      N=C(N)NC[C@@H]1COC2(CCCCC2)O1  1   \n",
      "1      Cefmetazole  CO[C@@]1(NC(=O)CSCC#N)C(=O)N2C(C(=O)O)=C(CSc3n...  0   \n",
      "2   Zonisamide.mol                           NS(=O)(=O)Cc1noc2ccccc12  1   \n",
      "3   Furosemide.mol            NS(=O)(=O)c1cc(Cl)cc(NCc2ccco2)c1C(=O)O  1   \n",
      "4  Telmisartan.mol  CCCc1nc2c(n1Cc1ccc(-c3ccccc3C(=O)O)cc1)=C[C@H]...  1   \n",
      "\n",
      "                                           Embedding  \n",
      "0  [0.24859753, 0.18472305, 0.4028932, 0.22700065...  \n",
      "1  [0.7069565, 0.41227153, 1.0127053, 2.3176281, ...  \n",
      "2  [0.19019875, -0.14087728, 0.8896561, 1.2718395...  \n",
      "3  [0.11933186, 0.38785577, 1.5808605, 1.999807, ...  \n",
      "4  [0.99853146, 1.1408926, 2.2468193, 1.3438487, ...  \n"
     ]
    }
   ],
   "source": [
    "print(mols_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Training a model\n",
    "Now that the molecules are featurised leverging the representation MiniMol learned during its pre-training, we will set up the training of a simple Multi-Layer Perceptron model on our newely generated embeddings and the labels from the `HIA Hou` dataset. We will use PyTorch.\n",
    "\n",
    "Let's start by defining a new class for the dataset and then creating the dataloaders for different splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "    \n",
    "class AdmetDataset(Dataset):\n",
    "    def __init__(self, samples):\n",
    "        self.samples = samples['Embedding'].tolist()\n",
    "        self.targets = [float(target) for target in samples['Y'].tolist()]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = torch.tensor(self.samples[idx])\n",
    "        target = torch.tensor(self.targets[idx])\n",
    "        return sample, target\n",
    "\n",
    "val_loader = DataLoader(AdmetDataset(mols_val), batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(AdmetDataset(mols_test), batch_size=128, shuffle=False)\n",
    "train_loader = DataLoader(AdmetDataset(mols_train), batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model will be a simple 3-layer perceptron with batch normalisation and dropout. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class TaskHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TaskHead, self).__init__()\n",
    "        self.dense1 = nn.Linear(512, 512)\n",
    "        self.dense2 = nn.Linear(512, 512)\n",
    "        self.dense3 = nn.Linear(512, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.dense2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.dense3(x)\n",
    "        return F.softmax(x)\n",
    "    \n",
    "\n",
    "predictor = TaskHead()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we declare the basic hyperparamters together with choosing optimiser, loss function, learning scheduler and weight decay regularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.001\n",
    "epochs = 10\n",
    "warmup = 5\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "lr_fn = lambda epoch: lr * (1 + math.cos(math.pi * (epoch - warmup) / (epochs - warmup))) / 2\n",
    "optimiser = optim.Adam(predictor.parameters(), weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimiser, lr_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start training, let's evaluate the randomly initilised model on the evaluation split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auroc =  0.5000\n",
      "avpr  =  0.8276\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "def evaluate(predictor, dataloader, loss_fn):\n",
    "    predictor.eval()\n",
    "    total_loss = 0\n",
    "    all_probs = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for inputs, targets in dataloader:\n",
    "            \n",
    "            probs = predictor(inputs.float())\n",
    "            \n",
    "            loss = loss_fn(output, targets.long())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            all_probs.extend(probs.squeeze().tolist())\n",
    "            all_targets.extend(targets.tolist())\n",
    "\n",
    "    loss = total_loss / len(dataloader)\n",
    "    return roc_auc_score(all_targets, all_probs), average_precision_score(all_targets, all_probs)\n",
    "\n",
    "\n",
    "auroc, avpr = evaluate(predictor, val_loader, loss_fn)\n",
    "print(\n",
    "    f\"auroc =  {auroc:.4f}\\n\"\n",
    "    f\"avpr  =  {avpr:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The randomly initialised task-head outputs values larger than >5 for all inputs, so after going through softmax, all classes are 1. Since the dataset is not class balanced, this model gets 82.8% average precisio, but AUROC, which measures both sensitivity and specificity, indicates that the model is no better than random. Let's see how good it gets after training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
