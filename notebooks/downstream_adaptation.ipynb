{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downstream adaption with MiniMol\n",
    "\n",
    "This example shows how MiniMol can featurise the molecules that will then serve as an input to another model trained on a small downstream dataset from TDC ADMET. This allows to transfer the knowledge from the pre-trained MiniMol to another task. \n",
    "\n",
    "Before we start, let's make sure that the TDC package is installed in the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install PyTDC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Getting the data\n",
    "Next, we will build a predictor for the `HIA Hou` dataset, one of the binary classification benchmarks corresponding to `absorption`-type of problems from TDC ADMET group. We then split the data into training, validation and test set based on molecular scaffolds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "generating training, validation splits...\n",
      "generating training, validation splits...\n",
      "100%|██████████| 461/461 [00:00<00:00, 3691.96it/s]\n"
     ]
    }
   ],
   "source": [
    "from tdc.benchmark_group import admet_group\n",
    "\n",
    "DATASET_NAME = 'hia_hou'\n",
    "\n",
    "admet = admet_group(path=\"admet-data/\")\n",
    "\n",
    "mols_test = admet.get(DATASET_NAME)['test']\n",
    "mols_train, mols_val = admet.get_train_valid_split(benchmark=DATASET_NAME, split_type='scaffold', seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset - hia_hou\n",
      "\n",
      "Val split (58 mols): \n",
      "                 Drug_ID                                               Drug  Y\n",
      "0         Atracurium.mol  COc1ccc(C[C@H]2c3cc(OC)c(OC)cc3CC[N@@+]2(C)CCC...  0\n",
      "1  Succinylsulfathiazole          O=C(O)CCC(=O)Nc1ccc(S(=O)(=O)Nc2nccs2)cc1  0\n",
      "2            Ticarcillin  CC1(C)S[C@H]2[C@@H](NC(=O)[C@@H](C(=O)O)c3ccsc...  0\n",
      "3          Raffinose.mol  OC[C@@H]1O[C@@H](OC[C@@H]2O[C@@H](O[C@]3(CO)O[...  0\n",
      "4          Triamcinolone  C[C@@]12C=CC(=O)C=C1CC[C@@H]1[C@H]3C[C@@H](O)[...  1\n",
      "\n",
      "Test split (117 mols): \n",
      "                Drug_ID                                               Drug  Y\n",
      "0         Trazodone.mol         O=c1n(CCCN2CCN(c3cccc(Cl)c3)CC2)nc2ccccn12  1\n",
      "1          Lisuride.mol  CCN(CC)C(=O)N[C@H]1C=C2c3cccc4[nH]cc(c34)C[C@@...  1\n",
      "2  Methylergonovine.mol  CC[C@H](CO)NC(=O)[C@H]1C=C2c3cccc4[nH]cc(c34)C...  1\n",
      "3      Methysergide.mol  CC[C@H](CO)NC(=O)[C@H]1C=C2c3cccc4c3c(cn4C)C[C...  1\n",
      "4       Moclobemide.mol                       O=C(NCCN1CCOCC1)c1ccc(Cl)cc1  1\n",
      "\n",
      "Train split (403 mols): \n",
      "           Drug_ID                                               Drug  Y\n",
      "0        Guanadrel                      N=C(N)NC[C@@H]1COC2(CCCCC2)O1  1\n",
      "1      Cefmetazole  CO[C@@]1(NC(=O)CSCC#N)C(=O)N2C(C(=O)O)=C(CSc3n...  0\n",
      "2   Zonisamide.mol                           NS(=O)(=O)Cc1noc2ccccc12  1\n",
      "3   Furosemide.mol            NS(=O)(=O)c1cc(Cl)cc(NCc2ccco2)c1C(=O)O  1\n",
      "4  Telmisartan.mol  CCCc1nc2c(n1Cc1ccc(-c3ccccc3C(=O)O)cc1)=C[C@H]...  1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset - {DATASET_NAME}\\n\")\n",
    "print(f\"Val split ({len(mols_val)} mols): \\n{mols_val.head()}\\n\")\n",
    "print(f\"Test split ({len(mols_test)} mols): \\n{mols_test.head()}\\n\")\n",
    "print(f\"Train split ({len(mols_train)} mols): \\n{mols_train.head()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generating molecular fingerprints\n",
    "After spltting the dataset into training, validation and test sets, we will use MiniMol to embed all molecules. The embedding will be added as an extra column in the dataframe returned by TDC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minimol import Minimol\n",
    "\n",
    "featuriser = Minimol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef0992ad7ec4997b5ff2a15095f9e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "featurizing_smiles, batch=1:   0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting to FP32: 100%|██████████| 58/58 [00:00<00:00, 8947.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.72it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd0a9b001794aaabd649bc26cb97d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "featurizing_smiles, batch=3:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting to FP32: 100%|██████████| 117/117 [00:00<00:00, 16518.57it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.86it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b20097911ae84991b2d35dec4ff6e6a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "featurizing_smiles, batch=13:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting to FP32: 100%|██████████| 403/403 [00:00<00:00, 19929.78it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00,  7.37it/s]\n"
     ]
    }
   ],
   "source": [
    "mols_val['Embedding'] = featuriser(mols_val['Drug'])\n",
    "mols_test['Embedding'] = featuriser(mols_test['Drug'])\n",
    "mols_train['Embedding'] = featuriser(mols_train['Drug'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is small, so it took us 7.3 seconds to generate the embeddings for almost 600 molecules. Here is a preview after a new column has been added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Drug_ID                                               Drug  Y  \\\n",
      "0        Guanadrel                      N=C(N)NC[C@@H]1COC2(CCCCC2)O1  1   \n",
      "1      Cefmetazole  CO[C@@]1(NC(=O)CSCC#N)C(=O)N2C(C(=O)O)=C(CSc3n...  0   \n",
      "2   Zonisamide.mol                           NS(=O)(=O)Cc1noc2ccccc12  1   \n",
      "3   Furosemide.mol            NS(=O)(=O)c1cc(Cl)cc(NCc2ccco2)c1C(=O)O  1   \n",
      "4  Telmisartan.mol  CCCc1nc2c(n1Cc1ccc(-c3ccccc3C(=O)O)cc1)=C[C@H]...  1   \n",
      "\n",
      "                                           Embedding  \n",
      "0  [0.24859753, 0.18472305, 0.4028932, 0.22700065...  \n",
      "1  [0.7069565, 0.41227153, 1.0127053, 2.3176281, ...  \n",
      "2  [0.19019875, -0.14087728, 0.8896561, 1.2718395...  \n",
      "3  [0.11933186, 0.38785577, 1.5808605, 1.999807, ...  \n",
      "4  [0.99853146, 1.1408926, 2.2468193, 1.3438487, ...  \n"
     ]
    }
   ],
   "source": [
    "print(mols_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Training a model\n",
    "Now that the molecules are featurised leverging the representation MiniMol learned during its pre-training, we will set up the training of a simple Multi-Layer Perceptron model on our newely generated embeddings and the labels from the `HIA Hou` dataset. We will use PyTorch.\n",
    "\n",
    "Let's start by defining a new class for the dataset and then creating the dataloaders for different splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "    \n",
    "class AdmetDataset(Dataset):\n",
    "    def __init__(self, samples):\n",
    "        self.samples = samples['Embedding'].tolist()\n",
    "        self.targets = [float(target) for target in samples['Y'].tolist()]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = torch.tensor(self.samples[idx])\n",
    "        target = torch.tensor(self.targets[idx])\n",
    "        return sample, target\n",
    "\n",
    "val_loader = DataLoader(AdmetDataset(mols_val), batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(AdmetDataset(mols_test), batch_size=128, shuffle=False)\n",
    "train_loader = DataLoader(AdmetDataset(mols_train), batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model will be a simple 3-layer perceptron with batch normalisation and dropout. Before the last layer the input features will be concatenated together with the output from the previous layer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class TaskHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TaskHead, self).__init__()\n",
    "        self.dense1 = nn.Linear(512, 512)\n",
    "        self.dense2 = nn.Linear(512, 512)\n",
    "        self.dense3 = nn.Linear(1024, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        original_x = x\n",
    "\n",
    "        x = self.dense1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.dense2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = torch.cat((x, original_x), dim=1)\n",
    "        x = self.dense3(x)\n",
    "        return self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we declare the basic hyperparamters together with choosing optimiser, loss function, learning scheduler and weight decay regularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.006\n",
    "epochs = 25\n",
    "warmup = 5\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "def model_factory():\n",
    "    model = TaskHead()\n",
    "    lr_fn = lambda epoch: lr * (1 + math.cos(math.pi * (epoch - warmup) / (epochs - warmup))) / 2\n",
    "    optimiser = optim.Adam(model.parameters(), lr=lr, weight_decay=0.0001)\n",
    "    lr_scheduler = optim.lr_scheduler.LambdaLR(optimiser, lr_fn)\n",
    "    return model, optimiser, lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluation we will use both AUROC and Average Precision metrics. The reported loss would be an average across all samples in the epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "def evaluate(predictor, dataloader, loss_fn):\n",
    "    predictor.eval()\n",
    "    total_loss = 0\n",
    "    all_probs = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for inputs, targets in dataloader:\n",
    "        \n",
    "            probs = predictor(inputs).squeeze()\n",
    "\n",
    "            loss = loss_fn(probs, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            all_probs.extend(probs.tolist())\n",
    "            all_targets.extend(targets.tolist())\n",
    "\n",
    "    loss = total_loss / len(dataloader)\n",
    "    \n",
    "    return (\n",
    "        loss,\n",
    "        roc_auc_score(all_targets, all_probs),\n",
    "        average_precision_score(all_targets, all_probs)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to define a method for training a model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(predictor, optimiser, lr_scheduler, loss_fn, epoch):\n",
    "    predictor.train()        \n",
    "    train_loss = 0\n",
    "\n",
    "    lr_scheduler.step(epoch)\n",
    "    for inputs, targets in train_loader:\n",
    "        optimiser.zero_grad()\n",
    "        probs = predictor(inputs).squeeze()\n",
    "        loss = loss_fn(probs, targets)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss, auroc, avpr = evaluate(predictor, val_loader, loss_fn)\n",
    "    print(\n",
    "        f\"## Epoch {epoch+1}\\t\"\n",
    "        f\"train_loss: {train_loss:.4f}\\t\"\n",
    "        f\"val_loss: {val_loss:.4f}\\t\"\n",
    "        f\"val_auroc: {auroc:.4f}\\t\"\n",
    "        f\"val_avpr: {avpr:.4f}\"\n",
    "    )\n",
    "    return predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's see how good our model gets after training... 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Epoch 0\ttrain_loss: ------\tval_loss: 0.6198\tval_auroc: 0.4712\tval_avpr: 0.8686\n",
      "## Epoch 1\ttrain_loss: 0.5763\tval_loss: 0.5906\tval_auroc: 0.4887\tval_avpr: 0.8722\n",
      "## Epoch 2\ttrain_loss: 0.4879\tval_loss: 0.5132\tval_auroc: 0.4536\tval_avpr: 0.8507\n",
      "## Epoch 3\ttrain_loss: 0.4125\tval_loss: 0.4404\tval_auroc: 0.4787\tval_avpr: 0.8584\n",
      "## Epoch 4\ttrain_loss: 0.3626\tval_loss: 0.3910\tval_auroc: 0.5439\tval_avpr: 0.8717\n",
      "## Epoch 5\ttrain_loss: 0.3205\tval_loss: 0.3605\tval_auroc: 0.5815\tval_avpr: 0.8935\n",
      "## Epoch 6\ttrain_loss: 0.2883\tval_loss: 0.3375\tval_auroc: 0.5990\tval_avpr: 0.8963\n",
      "## Epoch 7\ttrain_loss: 0.2585\tval_loss: 0.3202\tval_auroc: 0.6241\tval_avpr: 0.9073\n",
      "## Epoch 8\ttrain_loss: 0.2316\tval_loss: 0.3096\tval_auroc: 0.6516\tval_avpr: 0.9183\n",
      "## Epoch 9\ttrain_loss: 0.2167\tval_loss: 0.2989\tval_auroc: 0.6642\tval_avpr: 0.9222\n",
      "## Epoch 10\ttrain_loss: 0.2063\tval_loss: 0.2903\tval_auroc: 0.6842\tval_avpr: 0.9309\n",
      "## Epoch 11\ttrain_loss: 0.1929\tval_loss: 0.2828\tval_auroc: 0.7118\tval_avpr: 0.9422\n",
      "## Epoch 12\ttrain_loss: 0.1832\tval_loss: 0.2761\tval_auroc: 0.7168\tval_avpr: 0.9436\n",
      "## Epoch 13\ttrain_loss: 0.1705\tval_loss: 0.2734\tval_auroc: 0.7243\tval_avpr: 0.9444\n",
      "## Epoch 14\ttrain_loss: 0.1607\tval_loss: 0.2701\tval_auroc: 0.7293\tval_avpr: 0.9467\n",
      "## Epoch 15\ttrain_loss: 0.1604\tval_loss: 0.2674\tval_auroc: 0.7293\tval_avpr: 0.9456\n",
      "## Epoch 16\ttrain_loss: 0.1503\tval_loss: 0.2637\tval_auroc: 0.7494\tval_avpr: 0.9524\n",
      "## Epoch 17\ttrain_loss: 0.1424\tval_loss: 0.2621\tval_auroc: 0.7519\tval_avpr: 0.9528\n",
      "## Epoch 18\ttrain_loss: 0.1512\tval_loss: 0.2605\tval_auroc: 0.7519\tval_avpr: 0.9533\n",
      "## Epoch 19\ttrain_loss: 0.1445\tval_loss: 0.2576\tval_auroc: 0.7644\tval_avpr: 0.9561\n",
      "## Epoch 20\ttrain_loss: 0.1327\tval_loss: 0.2573\tval_auroc: 0.7619\tval_avpr: 0.9557\n",
      "## Epoch 21\ttrain_loss: 0.1407\tval_loss: 0.2564\tval_auroc: 0.7644\tval_avpr: 0.9561\n",
      "## Epoch 22\ttrain_loss: 0.1315\tval_loss: 0.2564\tval_auroc: 0.7669\tval_avpr: 0.9563\n",
      "## Epoch 23\ttrain_loss: 0.1344\tval_loss: 0.2548\tval_auroc: 0.7669\tval_avpr: 0.9568\n",
      "## Epoch 24\ttrain_loss: 0.1419\tval_loss: 0.2566\tval_auroc: 0.7644\tval_avpr: 0.9561\n",
      "## Epoch 25\ttrain_loss: 0.1381\tval_loss: 0.2564\tval_auroc: 0.7669\tval_avpr: 0.9568\n",
      "test_loss: 0.2680\n",
      "test_auroc: 0.9671\n",
      "test_avpr: 0.9893\n"
     ]
    }
   ],
   "source": [
    "model, optimiser, lr_scheduler = model_factory()\n",
    "\n",
    "val_loss, val_auroc, val_avpr = evaluate(model, val_loader, loss_fn)\n",
    "print(\n",
    "    f\"## Epoch 0\\t\"\n",
    "    f\"train_loss: ------\\t\"\n",
    "    f\"val_loss: {val_loss:.4f}\\t\"\n",
    "    f\"val_auroc: {val_auroc:.4f}\\t\"\n",
    "    f\"val_avpr: {val_avpr:.4f}\"\n",
    ")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model = train_one_epoch(model, optimiser, lr_scheduler, loss_fn, epoch)\n",
    "\n",
    "test_loss, test_auroc, test_avpr = evaluate(model, test_loader, loss_fn)\n",
    "print(\n",
    "    f\"test_loss: {test_loss:.4f}\\n\"\n",
    "    f\"test_auroc: {test_auroc:.4f}\\n\"\n",
    "    f\"test_avpr: {test_avpr:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model trained in just 1.4s, reaching AUROC on the test set of 0.9671. Pretty good!\n",
    "\n",
    "The result can be further improved. We adapt two techniques:\n",
    "\n",
    "- Ensembling. Since the training is so fast, fitting a few addtional models is not a big deal. We will train each of the five ensembled models on a different fold generated by the TDC train-val splitting method.\n",
    "\n",
    "- Rather than choosing the model at the last epoch, we will use best validation loss to decide which one to choose.\n",
    "\n",
    "Below we implement a method that create a new training and validation dataloader for each fold, and also a mehtod for evaluating using an ensemble rather than a single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader_factory(seed):\n",
    "    mols_train, mols_val = admet.get_train_valid_split(benchmark=DATASET_NAME, split_type='scaffold', seed=seed)\n",
    "\n",
    "    mols_val['Embedding'] = featuriser(mols_val['Drug'])\n",
    "    mols_train['Embedding'] = featuriser(mols_train['Drug'])\n",
    "\n",
    "    val_loader = DataLoader(AdmetDataset(mols_val), batch_size=128, shuffle=False)\n",
    "    train_loader = DataLoader(AdmetDataset(mols_train), batch_size=32, shuffle=True)\n",
    "\n",
    "    return val_loader, train_loader\n",
    "\n",
    "\n",
    "def evaluate_ensemble(predictors, dataloader, loss_fn):\n",
    "    total_loss = 0\n",
    "    all_probs = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for inputs, targets in dataloader:\n",
    "            model_outputs = [predictor(inputs).squeeze() for predictor in predictors]\n",
    "            averaged_output = torch.mean(torch.stack(model_outputs, dim=0), dim=0)\n",
    "\n",
    "            loss = loss_fn(averaged_output, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            all_probs.extend(averaged_output.tolist())\n",
    "            all_targets.extend(targets.tolist())\n",
    "\n",
    "    loss = total_loss / len(dataloader)\n",
    "    return loss, roc_auc_score(all_targets, all_probs), average_precision_score(all_targets, all_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's see how much better the model can get!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating training, validation splits...\n",
      "100%|██████████| 461/461 [00:00<00:00, 3196.61it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "872e0b43fa954a18a2435dd2092ee3bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "featurizing_smiles, batch=1:   0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting to FP32: 100%|██████████| 58/58 [00:00<00:00, 15806.99it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.36it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ad695e92784e8fa526da28ce1bcbf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "featurizing_smiles, batch=13:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting to FP32: 100%|██████████| 403/403 [00:00<00:00, 16046.94it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Epoch 1\ttrain_loss: 0.5566\tval_loss: 0.5515\tval_auroc: 0.3081\tval_avpr: 0.8305\n",
      "## Epoch 2\ttrain_loss: 0.4675\tval_loss: 0.4924\tval_auroc: 0.5826\tval_avpr: 0.8775\n",
      "## Epoch 3\ttrain_loss: 0.4083\tval_loss: 0.4274\tval_auroc: 0.6947\tval_avpr: 0.9211\n",
      "## Epoch 4\ttrain_loss: 0.3606\tval_loss: 0.3802\tval_auroc: 0.7535\tval_avpr: 0.9460\n",
      "## Epoch 5\ttrain_loss: 0.3154\tval_loss: 0.3475\tval_auroc: 0.7675\tval_avpr: 0.9505\n",
      "## Epoch 6\ttrain_loss: 0.2842\tval_loss: 0.3227\tval_auroc: 0.8151\tval_avpr: 0.9658\n",
      "## Epoch 7\ttrain_loss: 0.2555\tval_loss: 0.3013\tval_auroc: 0.8459\tval_avpr: 0.9737\n",
      "## Epoch 8\ttrain_loss: 0.2338\tval_loss: 0.2869\tval_auroc: 0.8543\tval_avpr: 0.9754\n",
      "## Epoch 9\ttrain_loss: 0.2137\tval_loss: 0.2747\tval_auroc: 0.8711\tval_avpr: 0.9793\n",
      "## Epoch 10\ttrain_loss: 0.2000\tval_loss: 0.2675\tval_auroc: 0.8796\tval_avpr: 0.9810\n",
      "## Epoch 11\ttrain_loss: 0.1892\tval_loss: 0.2595\tval_auroc: 0.8824\tval_avpr: 0.9816\n",
      "## Epoch 12\ttrain_loss: 0.1900\tval_loss: 0.2528\tval_auroc: 0.8964\tval_avpr: 0.9844\n",
      "## Epoch 13\ttrain_loss: 0.1725\tval_loss: 0.2486\tval_auroc: 0.8992\tval_avpr: 0.9848\n",
      "## Epoch 14\ttrain_loss: 0.1637\tval_loss: 0.2450\tval_auroc: 0.9020\tval_avpr: 0.9853\n",
      "## Epoch 15\ttrain_loss: 0.1584\tval_loss: 0.2418\tval_auroc: 0.9020\tval_avpr: 0.9853\n",
      "## Epoch 16\ttrain_loss: 0.1487\tval_loss: 0.2388\tval_auroc: 0.9104\tval_avpr: 0.9868\n",
      "## Epoch 17\ttrain_loss: 0.1476\tval_loss: 0.2357\tval_auroc: 0.9104\tval_avpr: 0.9868\n",
      "## Epoch 18\ttrain_loss: 0.1486\tval_loss: 0.2342\tval_auroc: 0.9104\tval_avpr: 0.9868\n",
      "## Epoch 19\ttrain_loss: 0.1381\tval_loss: 0.2339\tval_auroc: 0.9104\tval_avpr: 0.9868\n",
      "## Epoch 20\ttrain_loss: 0.1400\tval_loss: 0.2323\tval_auroc: 0.9132\tval_avpr: 0.9873\n",
      "## Epoch 21\ttrain_loss: 0.1375\tval_loss: 0.2312\tval_auroc: 0.9132\tval_avpr: 0.9873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating training, validation splits...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Epoch 22\ttrain_loss: 0.1294\tval_loss: 0.2317\tval_auroc: 0.9132\tval_avpr: 0.9873\n",
      "## Epoch 23\ttrain_loss: 0.1304\tval_loss: 0.2315\tval_auroc: 0.9132\tval_avpr: 0.9873\n",
      "## Epoch 24\ttrain_loss: 0.1335\tval_loss: 0.2314\tval_auroc: 0.9132\tval_avpr: 0.9873\n",
      "## Epoch 25\ttrain_loss: 0.1281\tval_loss: 0.2307\tval_auroc: 0.9132\tval_avpr: 0.9873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 461/461 [00:00<00:00, 3069.32it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ee9e6ee57540ccadf829b6ac24a007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "featurizing_smiles, batch=1:   0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting to FP32: 100%|██████████| 58/58 [00:00<00:00, 17746.54it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.08it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cec3d6fd0934ad9a3c0bc87d773d980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "featurizing_smiles, batch=13:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting to FP32: 100%|██████████| 403/403 [00:00<00:00, 19709.71it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Epoch 1\ttrain_loss: 0.4981\tval_loss: 0.5801\tval_auroc: 0.4528\tval_avpr: 0.9011\n",
      "## Epoch 2\ttrain_loss: 0.4269\tval_loss: 0.4856\tval_auroc: 0.6491\tval_avpr: 0.9385\n",
      "## Epoch 3\ttrain_loss: 0.3687\tval_loss: 0.3989\tval_auroc: 0.7660\tval_avpr: 0.9677\n",
      "## Epoch 4\ttrain_loss: 0.3297\tval_loss: 0.3509\tval_auroc: 0.8113\tval_avpr: 0.9759\n",
      "## Epoch 5\ttrain_loss: 0.2891\tval_loss: 0.3164\tval_auroc: 0.8226\tval_avpr: 0.9780\n",
      "## Epoch 6\ttrain_loss: 0.2658\tval_loss: 0.2861\tval_auroc: 0.8491\tval_avpr: 0.9819\n",
      "## Epoch 7\ttrain_loss: 0.2421\tval_loss: 0.2669\tval_auroc: 0.8755\tval_avpr: 0.9858\n",
      "## Epoch 8\ttrain_loss: 0.2309\tval_loss: 0.2479\tval_auroc: 0.8830\tval_avpr: 0.9868\n",
      "## Epoch 9\ttrain_loss: 0.2134\tval_loss: 0.2383\tval_auroc: 0.8792\tval_avpr: 0.9866\n",
      "## Epoch 10\ttrain_loss: 0.1947\tval_loss: 0.2265\tval_auroc: 0.8906\tval_avpr: 0.9878\n",
      "## Epoch 11\ttrain_loss: 0.1809\tval_loss: 0.2145\tval_auroc: 0.8981\tval_avpr: 0.9889\n",
      "## Epoch 12\ttrain_loss: 0.1672\tval_loss: 0.2115\tval_auroc: 0.9132\tval_avpr: 0.9909\n",
      "## Epoch 13\ttrain_loss: 0.1681\tval_loss: 0.2015\tval_auroc: 0.9208\tval_avpr: 0.9918\n",
      "## Epoch 14\ttrain_loss: 0.1631\tval_loss: 0.1969\tval_auroc: 0.9283\tval_avpr: 0.9927\n",
      "## Epoch 15\ttrain_loss: 0.1486\tval_loss: 0.1937\tval_auroc: 0.9358\tval_avpr: 0.9936\n",
      "## Epoch 16\ttrain_loss: 0.1467\tval_loss: 0.1913\tval_auroc: 0.9358\tval_avpr: 0.9936\n",
      "## Epoch 17\ttrain_loss: 0.1413\tval_loss: 0.1886\tval_auroc: 0.9396\tval_avpr: 0.9939\n",
      "## Epoch 18\ttrain_loss: 0.1439\tval_loss: 0.1867\tval_auroc: 0.9396\tval_avpr: 0.9939\n",
      "## Epoch 19\ttrain_loss: 0.1406\tval_loss: 0.1830\tval_auroc: 0.9396\tval_avpr: 0.9939\n",
      "## Epoch 20\ttrain_loss: 0.1325\tval_loss: 0.1804\tval_auroc: 0.9396\tval_avpr: 0.9939\n",
      "## Epoch 21\ttrain_loss: 0.1321\tval_loss: 0.1779\tval_auroc: 0.9396\tval_avpr: 0.9939\n",
      "## Epoch 22\ttrain_loss: 0.1359\tval_loss: 0.1807\tval_auroc: 0.9396\tval_avpr: 0.9939\n",
      "## Epoch 23\ttrain_loss: 0.1394\tval_loss: 0.1788\tval_auroc: 0.9434\tval_avpr: 0.9943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating training, validation splits...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Epoch 24\ttrain_loss: 0.1306\tval_loss: 0.1802\tval_auroc: 0.9434\tval_avpr: 0.9944\n",
      "## Epoch 25\ttrain_loss: 0.1301\tval_loss: 0.1778\tval_auroc: 0.9434\tval_avpr: 0.9943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 461/461 [00:00<00:00, 3789.72it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7084adf3ac8e4edb878206def84b0678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "featurizing_smiles, batch=1:   0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting to FP32: 100%|██████████| 58/58 [00:00<00:00, 14143.58it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.68it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ca07a063c224d6984e2a69ca1b7bc37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "featurizing_smiles, batch=13:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting to FP32: 100%|██████████| 403/403 [00:00<00:00, 16329.24it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00,  5.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Epoch 1\ttrain_loss: 0.5888\tval_loss: 0.5955\tval_auroc: 0.3109\tval_avpr: 0.8351\n",
      "## Epoch 2\ttrain_loss: 0.5061\tval_loss: 0.5120\tval_auroc: 0.6603\tval_avpr: 0.9407\n",
      "## Epoch 3\ttrain_loss: 0.4270\tval_loss: 0.4331\tval_auroc: 0.8333\tval_avpr: 0.9703\n",
      "## Epoch 4\ttrain_loss: 0.3892\tval_loss: 0.3694\tval_auroc: 0.8622\tval_avpr: 0.9780\n",
      "## Epoch 5\ttrain_loss: 0.3384\tval_loss: 0.3416\tval_auroc: 0.8782\tval_avpr: 0.9813\n",
      "## Epoch 6\ttrain_loss: 0.3086\tval_loss: 0.3090\tval_auroc: 0.9071\tval_avpr: 0.9875\n",
      "## Epoch 7\ttrain_loss: 0.2751\tval_loss: 0.2914\tval_auroc: 0.9071\tval_avpr: 0.9873\n",
      "## Epoch 8\ttrain_loss: 0.2534\tval_loss: 0.2720\tval_auroc: 0.9263\tval_avpr: 0.9906\n",
      "## Epoch 9\ttrain_loss: 0.2409\tval_loss: 0.2577\tval_auroc: 0.9359\tval_avpr: 0.9919\n",
      "## Epoch 10\ttrain_loss: 0.2148\tval_loss: 0.2464\tval_auroc: 0.9455\tval_avpr: 0.9933\n",
      "## Epoch 11\ttrain_loss: 0.2104\tval_loss: 0.2390\tval_auroc: 0.9487\tval_avpr: 0.9938\n",
      "## Epoch 12\ttrain_loss: 0.1975\tval_loss: 0.2297\tval_auroc: 0.9583\tval_avpr: 0.9951\n",
      "## Epoch 13\ttrain_loss: 0.1904\tval_loss: 0.2242\tval_auroc: 0.9583\tval_avpr: 0.9951\n",
      "## Epoch 14\ttrain_loss: 0.1819\tval_loss: 0.2178\tval_auroc: 0.9583\tval_avpr: 0.9951\n",
      "## Epoch 15\ttrain_loss: 0.1724\tval_loss: 0.2137\tval_auroc: 0.9615\tval_avpr: 0.9955\n",
      "## Epoch 16\ttrain_loss: 0.1651\tval_loss: 0.2105\tval_auroc: 0.9647\tval_avpr: 0.9959\n",
      "## Epoch 17\ttrain_loss: 0.1624\tval_loss: 0.2071\tval_auroc: 0.9647\tval_avpr: 0.9959\n",
      "## Epoch 18\ttrain_loss: 0.1579\tval_loss: 0.2050\tval_auroc: 0.9647\tval_avpr: 0.9959\n",
      "## Epoch 19\ttrain_loss: 0.1561\tval_loss: 0.2037\tval_auroc: 0.9647\tval_avpr: 0.9959\n",
      "## Epoch 20\ttrain_loss: 0.1536\tval_loss: 0.2036\tval_auroc: 0.9679\tval_avpr: 0.9963\n",
      "## Epoch 21\ttrain_loss: 0.1471\tval_loss: 0.2018\tval_auroc: 0.9712\tval_avpr: 0.9966\n",
      "## Epoch 22\ttrain_loss: 0.1465\tval_loss: 0.2008\tval_auroc: 0.9712\tval_avpr: 0.9966\n",
      "## Epoch 23\ttrain_loss: 0.1526\tval_loss: 0.2006\tval_auroc: 0.9679\tval_avpr: 0.9963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating training, validation splits...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Epoch 24\ttrain_loss: 0.1524\tval_loss: 0.1998\tval_auroc: 0.9712\tval_avpr: 0.9966\n",
      "## Epoch 25\ttrain_loss: 0.1449\tval_loss: 0.1999\tval_auroc: 0.9712\tval_avpr: 0.9966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 461/461 [00:00<00:00, 3792.48it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d190c106804d01bd9664f6032b7a4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "featurizing_smiles, batch=1:   0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting to FP32: 100%|██████████| 58/58 [00:00<00:00, 15222.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.83it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b590c981bf7b49cba2623b6f5c30e0b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "featurizing_smiles, batch=13:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting to FP32: 100%|██████████| 403/403 [00:00<00:00, 14800.75it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00,  8.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Epoch 1\ttrain_loss: 0.5502\tval_loss: 0.5522\tval_auroc: 0.6065\tval_avpr: 0.9563\n",
      "## Epoch 2\ttrain_loss: 0.4692\tval_loss: 0.4540\tval_auroc: 0.7593\tval_avpr: 0.9737\n",
      "## Epoch 3\ttrain_loss: 0.4013\tval_loss: 0.3752\tval_auroc: 0.7917\tval_avpr: 0.9728\n",
      "## Epoch 4\ttrain_loss: 0.3501\tval_loss: 0.3212\tval_auroc: 0.8102\tval_avpr: 0.9769\n",
      "## Epoch 5\ttrain_loss: 0.3182\tval_loss: 0.2918\tval_auroc: 0.8148\tval_avpr: 0.9780\n",
      "## Epoch 6\ttrain_loss: 0.2813\tval_loss: 0.2605\tval_auroc: 0.8565\tval_avpr: 0.9857\n",
      "## Epoch 7\ttrain_loss: 0.2536\tval_loss: 0.2479\tval_auroc: 0.8565\tval_avpr: 0.9860\n",
      "## Epoch 8\ttrain_loss: 0.2370\tval_loss: 0.2274\tval_auroc: 0.8657\tval_avpr: 0.9873\n",
      "## Epoch 9\ttrain_loss: 0.2189\tval_loss: 0.2134\tval_auroc: 0.8750\tval_avpr: 0.9883\n",
      "## Epoch 10\ttrain_loss: 0.2053\tval_loss: 0.2036\tval_auroc: 0.8704\tval_avpr: 0.9877\n",
      "## Epoch 11\ttrain_loss: 0.1918\tval_loss: 0.1961\tval_auroc: 0.8889\tval_avpr: 0.9900\n",
      "## Epoch 12\ttrain_loss: 0.1851\tval_loss: 0.1913\tval_auroc: 0.8981\tval_avpr: 0.9911\n",
      "## Epoch 13\ttrain_loss: 0.1773\tval_loss: 0.1863\tval_auroc: 0.9028\tval_avpr: 0.9916\n",
      "## Epoch 14\ttrain_loss: 0.1748\tval_loss: 0.1838\tval_auroc: 0.9028\tval_avpr: 0.9916\n",
      "## Epoch 15\ttrain_loss: 0.1622\tval_loss: 0.1796\tval_auroc: 0.9028\tval_avpr: 0.9916\n",
      "## Epoch 16\ttrain_loss: 0.1542\tval_loss: 0.1798\tval_auroc: 0.9074\tval_avpr: 0.9921\n",
      "## Epoch 17\ttrain_loss: 0.1482\tval_loss: 0.1722\tval_auroc: 0.9074\tval_avpr: 0.9921\n",
      "## Epoch 18\ttrain_loss: 0.1400\tval_loss: 0.1740\tval_auroc: 0.9074\tval_avpr: 0.9921\n",
      "## Epoch 19\ttrain_loss: 0.1397\tval_loss: 0.1727\tval_auroc: 0.9074\tval_avpr: 0.9921\n",
      "## Epoch 20\ttrain_loss: 0.1431\tval_loss: 0.1697\tval_auroc: 0.9213\tval_avpr: 0.9935\n",
      "## Epoch 21\ttrain_loss: 0.1379\tval_loss: 0.1697\tval_auroc: 0.9259\tval_avpr: 0.9940\n",
      "## Epoch 22\ttrain_loss: 0.1392\tval_loss: 0.1674\tval_auroc: 0.9259\tval_avpr: 0.9940\n",
      "## Epoch 23\ttrain_loss: 0.1304\tval_loss: 0.1699\tval_auroc: 0.9259\tval_avpr: 0.9940\n",
      "## Epoch 24\ttrain_loss: 0.1357\tval_loss: 0.1716\tval_auroc: 0.9074\tval_avpr: 0.9921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating training, validation splits...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Epoch 25\ttrain_loss: 0.1377\tval_loss: 0.1738\tval_auroc: 0.9074\tval_avpr: 0.9921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 461/461 [00:00<00:00, 3762.20it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46fe774421074e20a38fecb88b13360e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "featurizing_smiles, batch=2:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting to FP32: 100%|██████████| 64/64 [00:00<00:00, 14105.17it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.85it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a9313a1c27a41e285ec9feb9dfe9ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "featurizing_smiles, batch=13:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting to FP32: 100%|██████████| 397/397 [00:00<00:00, 17897.21it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Epoch 1\ttrain_loss: 0.5709\tval_loss: 0.5547\tval_auroc: 0.5263\tval_avpr: 0.9043\n",
      "## Epoch 2\ttrain_loss: 0.4856\tval_loss: 0.4772\tval_auroc: 0.6992\tval_avpr: 0.9433\n",
      "## Epoch 3\ttrain_loss: 0.4175\tval_loss: 0.4074\tval_auroc: 0.7419\tval_avpr: 0.9431\n",
      "## Epoch 4\ttrain_loss: 0.3627\tval_loss: 0.3590\tval_auroc: 0.7494\tval_avpr: 0.9491\n",
      "## Epoch 5\ttrain_loss: 0.3160\tval_loss: 0.3317\tval_auroc: 0.7694\tval_avpr: 0.9572\n",
      "## Epoch 6\ttrain_loss: 0.2816\tval_loss: 0.3095\tval_auroc: 0.7694\tval_avpr: 0.9571\n",
      "## Epoch 7\ttrain_loss: 0.2555\tval_loss: 0.2954\tval_auroc: 0.7719\tval_avpr: 0.9590\n",
      "## Epoch 8\ttrain_loss: 0.2365\tval_loss: 0.2838\tval_auroc: 0.7870\tval_avpr: 0.9633\n",
      "## Epoch 9\ttrain_loss: 0.2165\tval_loss: 0.2749\tval_auroc: 0.8095\tval_avpr: 0.9688\n",
      "## Epoch 10\ttrain_loss: 0.2049\tval_loss: 0.2682\tval_auroc: 0.8246\tval_avpr: 0.9724\n",
      "## Epoch 11\ttrain_loss: 0.1894\tval_loss: 0.2615\tval_auroc: 0.8271\tval_avpr: 0.9727\n",
      "## Epoch 12\ttrain_loss: 0.1848\tval_loss: 0.2569\tval_auroc: 0.8371\tval_avpr: 0.9748\n",
      "## Epoch 13\ttrain_loss: 0.1665\tval_loss: 0.2539\tval_auroc: 0.8396\tval_avpr: 0.9752\n",
      "## Epoch 14\ttrain_loss: 0.1737\tval_loss: 0.2506\tval_auroc: 0.8521\tval_avpr: 0.9777\n",
      "## Epoch 15\ttrain_loss: 0.1592\tval_loss: 0.2465\tval_auroc: 0.8596\tval_avpr: 0.9787\n",
      "## Epoch 16\ttrain_loss: 0.1446\tval_loss: 0.2431\tval_auroc: 0.8496\tval_avpr: 0.9766\n",
      "## Epoch 17\ttrain_loss: 0.1480\tval_loss: 0.2415\tval_auroc: 0.8647\tval_avpr: 0.9794\n",
      "## Epoch 18\ttrain_loss: 0.1484\tval_loss: 0.2420\tval_auroc: 0.8672\tval_avpr: 0.9797\n",
      "## Epoch 19\ttrain_loss: 0.1437\tval_loss: 0.2397\tval_auroc: 0.8747\tval_avpr: 0.9816\n",
      "## Epoch 20\ttrain_loss: 0.1321\tval_loss: 0.2382\tval_auroc: 0.8722\tval_avpr: 0.9815\n",
      "## Epoch 21\ttrain_loss: 0.1435\tval_loss: 0.2374\tval_auroc: 0.8797\tval_avpr: 0.9823\n",
      "## Epoch 22\ttrain_loss: 0.1351\tval_loss: 0.2364\tval_auroc: 0.8847\tval_avpr: 0.9832\n",
      "## Epoch 23\ttrain_loss: 0.1403\tval_loss: 0.2343\tval_auroc: 0.8672\tval_avpr: 0.9797\n",
      "## Epoch 24\ttrain_loss: 0.1551\tval_loss: 0.2369\tval_auroc: 0.8822\tval_avpr: 0.9829\n",
      "## Epoch 25\ttrain_loss: 0.1354\tval_loss: 0.2353\tval_auroc: 0.8747\tval_avpr: 0.9813\n",
      "test_loss: 0.2548\n",
      "test_auroc: 0.9831\n",
      "test_avpr: 0.9950\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "seeds = [1, 2, 3, 4, 5]\n",
    "\n",
    "best_models = []\n",
    "num_folds = 5\n",
    "\n",
    "for seed in seeds:\n",
    "    val_loader, train_loader = dataloader_factory(seed)\n",
    "    model, optimiser, lr_scheduler = model_factory()\n",
    "\n",
    "    best_epoch = {\"model\": None, \"result\": None}\n",
    "    for epoch in range(epochs):\n",
    "        model = train_one_epoch(model, optimiser, lr_scheduler, loss_fn, epoch)\n",
    "        val_loss, auroc, _ = evaluate(model, val_loader, loss_fn)\n",
    "\n",
    "        if best_epoch['model'] is None:\n",
    "            best_epoch['model'] = deepcopy(model)\n",
    "            best_epoch['result'] = auroc\n",
    "        else:\n",
    "            best_epoch['model'] = best_epoch['model'] if best_epoch['result'] <= val_loss else deepcopy(model)\n",
    "            best_epoch['result'] = best_epoch['result'] if best_epoch['result'] <= val_loss else val_loss \n",
    "\n",
    "    best_models.append(best_epoch['model'])\n",
    "\n",
    "test_loss, test_auroc, test_avpr = evaluate_ensemble(best_models, test_loader, loss_fn)\n",
    "print(\n",
    "    f\"test_loss: {test_loss:.4f}\\n\"\n",
    "    f\"test_auroc: {test_auroc:.4f}\\n\"\n",
    "    f\"test_avpr: {test_avpr:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, in less than 15s our performance improved from 0.9671 to 0.9831 AUROC on the test set. \n",
    "\n",
    "The results could be further improved by doing a hyperparameter sweep for the task-specific head. Considering low cost of fitting these tiny models, it would still be very fast. The results reported in the paper are slighly better than what we show here, thanks to sweeping that we performed for each task, getting us a few extra percent points of boost.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
